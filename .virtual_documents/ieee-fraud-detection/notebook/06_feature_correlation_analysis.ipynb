


# import  library and load data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-whitegrid')

train = pd.read_parquet(Path('../data/interim/train_merged.parquet'))
print(f'Data loaded: {train.shape}')


# numeric features
numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [c for c in numeric_cols if c not in ['Transactionid', 'isFraud']]

print(f'Numeric features : {len(numeric_cols)}')

# sample for speed
sample = train[numeric_cols].sample(n=50000, random_state=42)


# V FEATURE CORRELATION
v_cols = [c for c in numeric_cols if c.startswith('V')]
print(f'v-features : {len(v_cols)}')

# compute correaltion matrix
v_corr = sample[v_cols].corr()

# find high correlation pairs
high_corr_pairs = []
for i in range(len(v_cols)):
    for j in range(1+1, len(v_cols)):
        if abs(v_corr.iloc[i, j ]) > 0.95:
            high_corr_pairs.append((v_cols[i], v_cols[j], v_corr.iloc[i,j]))


print(f'Highly correlated pairs |r| > 95 : {len(high_corr_pairs)}')
print('\nSample pairs :')
for f1, f2, r in high_corr_pairs[:10]:
    print(f'{f1} - {f2} : {r:.3f}')





# identify redundant features 

# target correaltion
target_corr = sample.join(train['isFraud']).corr()['isFraud'].drop('isFraud').abs()

# for each correalted pairm mark the one with lower target correaltionfor removal
to_remove = set()
for f1, f2, r in high_corr_pairs:
    if target_corr.get(f1, 0) < target_corr.get(f2, 0):
        to_remove.add(f1)
    else:
        to_remove.add(f2)
