{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29653d6d",
   "metadata": {},
   "source": [
    "## Feature correlation analysis\n",
    "\n",
    "## Context\n",
    "\n",
    "with 400 + features, many derived from similar signals, redudancy is inevitable. Highly correlated features add noise, slow down training, and can destabilize linear model. this analysis identifies clusters of correlated features for pruning\n",
    "\n",
    "## Objective \n",
    "- compute correaltion metrices for numeric features\n",
    "- identify highly correlated feature paris(|r| > 0.95)\n",
    "- select candidates for removal based on correalatin and importance\n",
    "- quantify potential dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c6ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (590540, 434)\n"
     ]
    }
   ],
   "source": [
    "# import  library and load data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "train = pd.read_parquet(Path('../data/interim/train_merged.parquet'))\n",
    "print(f'Data loaded: {train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f01e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features : 402\n"
     ]
    }
   ],
   "source": [
    "# numeric features\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c not in ['Transactionid', 'isFraud']]\n",
    "\n",
    "print(f'Numeric features : {len(numeric_cols)}')\n",
    "\n",
    "# sample for speed\n",
    "sample = train[numeric_cols].sample(n=50000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345a5fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v-features : 339\n",
      "Highly correlated pairs |r| > 95 : 634\n",
      "\n",
      "Sample pairs :\n",
      "V10 - V11 : 0.970\n",
      "V15 - V16 : 0.985\n",
      "V15 - V33 : 0.956\n",
      "V15 - V57 : 0.954\n",
      "V15 - V94 : 0.953\n",
      "V17 - V18 : 0.994\n",
      "V17 - V21 : 0.959\n",
      "V18 - V21 : 0.954\n",
      "V21 - V22 : 0.965\n",
      "V21 - V84 : 0.956\n"
     ]
    }
   ],
   "source": [
    "# V FEATURE CORRELATION\n",
    "v_cols = [c for c in numeric_cols if c.startswith('V')]\n",
    "print(f'v-features : {len(v_cols)}')\n",
    "\n",
    "# compute correaltion matrix\n",
    "v_corr = sample[v_cols].corr()\n",
    "\n",
    "# find high correlation pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(v_cols)):\n",
    "    for j in range(i+1, len(v_cols)):\n",
    "        if abs(v_corr.iloc[i, j ]) > 0.95:\n",
    "            high_corr_pairs.append((v_cols[i], v_cols[j], v_corr.iloc[i,j]))\n",
    "\n",
    "\n",
    "print(f'Highly correlated pairs |r| > 95 : {len(high_corr_pairs)}')\n",
    "print('\\nSample pairs :')\n",
    "for f1, f2, r in high_corr_pairs[:10]:\n",
    "    print(f'{f1} - {f2} : {r:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17754522",
   "metadata": {},
   "source": [
    "## insight : V-features correlations clusters\n",
    "\n",
    "from the correlation anyallysis , we identify dense clusters of redudancy :\n",
    "- many v-features paris exceed r > 0.95 whic indicates near perfect correaltion\n",
    "- these likely com from similiar aggregation windows or realted transformations in vesta's system\n",
    "- keeping all of them adds noise and training overhead without imporving AUC\n",
    "\n",
    "pruning strategy : for each correalted pair, drop the feature with lower target correlation or higher missingness to preserve the most predictive signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a96059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Recomended for removal : 130\n",
      "Original V-features : 339\n",
      "After pruning : 209\n",
      "reduction : 38.3%\n"
     ]
    }
   ],
   "source": [
    "# identify redundant features \n",
    "\n",
    "# target correaltion\n",
    "target_corr = sample.join(train['isFraud']).corr()['isFraud'].drop('isFraud').abs()\n",
    "\n",
    "# for each correalted pairm mark the one with lower target correaltionfor removal\n",
    "to_remove = set()\n",
    "for f1, f2, r in high_corr_pairs:\n",
    "    if target_corr.get(f1, 0) < target_corr.get(f2, 0):\n",
    "        to_remove.add(f1)\n",
    "    else:\n",
    "        to_remove.add(f2)\n",
    "        \n",
    "print(f'Features Recomended for removal : {len(to_remove)}')\n",
    "print(f'Original V-features : {len(v_cols)}')\n",
    "print(f'After pruning : {len(v_cols) - len(to_remove)}')\n",
    "print(f'reduction : {len(to_remove)/ len(v_cols)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70916119",
   "metadata": {},
   "source": [
    "## Insight : Dimensionality Reduction IMpact\n",
    "\n",
    "from redudancy analysis:\n",
    "- we can potentially remove 50 + feature with correlation exceeding 0.95\n",
    "- this represent approximately 15% reduciton in feature space\n",
    "- training time decreases proportionally with minimal auc loss ( typically les than 0.1%)\n",
    "\n",
    "balance : aggresive pruning speeds up iteration cycles while conservative pruning preserves potential signal. start aggresive, then add features back if auc suffer noticeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac6bcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved redundant features list to: ..\\data\\metadata\\redundant_feature.csv\n",
      "features to drop 130\n"
     ]
    }
   ],
   "source": [
    "# save redundant features\n",
    "\n",
    "output_path = Path('../data/metadata')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.Series(list(to_remove)).to_csv(output_path / 'redundant_feature.csv', index=False)\n",
    "print(f'Saved redundant features list to: {output_path / 'redundant_feature.csv'}')\n",
    "print(f'features to drop {len(to_remove)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10585648",
   "metadata": {},
   "source": [
    "## key takeways \n",
    "\n",
    "from the features correlation analysis :\n",
    "- redundancy is substantial : we identified 50+ features pairs with > 0.95 correaltion\n",
    "- v_features are the main sources : the vesta engineered V1-V339 features contains many near duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
