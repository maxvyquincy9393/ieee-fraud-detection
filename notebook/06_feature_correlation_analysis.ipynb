{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29653d6d",
   "metadata": {},
   "source": [
    "## Feature correlation analysis\n",
    "\n",
    "## Context\n",
    "\n",
    "with 400 + features, manu derived from similar signals, redudancy is inevitable. Highly correlated features add noise, slow down training, and can destabilize linear model. this analysis identifies clusters of correlated features for pruning\n",
    "\n",
    "## Objective \n",
    "- compute correaltion metrices for numeric features\n",
    "- identify highly correlated feature paris(|r| > 0.95)\n",
    "- select candidates for removal based on correalatin and importance\n",
    "- quantify potential dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c6ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (590540, 434)\n"
     ]
    }
   ],
   "source": [
    "# import  library and load data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "train = pd.read_parquet(Path('../data/interim/train_merged.parquet'))\n",
    "print(f'Data loaded: {train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f01e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features : 402\n"
     ]
    }
   ],
   "source": [
    "# numeric features\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c not in ['Transactionid', 'isFraud']]\n",
    "\n",
    "print(f'Numeric features : {len(numeric_cols)}')\n",
    "\n",
    "# sample for speed\n",
    "sample = train[numeric_cols].sample(n=50000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345a5fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v-features : 339\n",
      "Highly correlated pairs |r| > 95 : 1604\n",
      "\n",
      "Sample pairs :\n",
      "V3 - V3 : 1.000\n",
      "V4 - V4 : 1.000\n",
      "V5 - V5 : 1.000\n",
      "V6 - V6 : 1.000\n",
      "V7 - V7 : 1.000\n",
      "V8 - V8 : 1.000\n",
      "V9 - V9 : 1.000\n",
      "V10 - V10 : 1.000\n",
      "V10 - V11 : 0.970\n",
      "V11 - V10 : 0.970\n"
     ]
    }
   ],
   "source": [
    "# V FEATURE CORRELATION\n",
    "v_cols = [c for c in numeric_cols if c.startswith('V')]\n",
    "print(f'v-features : {len(v_cols)}')\n",
    "\n",
    "# compute correaltion matrix\n",
    "v_corr = sample[v_cols].corr()\n",
    "\n",
    "# find high correlation pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(v_cols)):\n",
    "    for j in range(1+1, len(v_cols)):\n",
    "        if abs(v_corr.iloc[i, j ]) > 0.95:\n",
    "            high_corr_pairs.append((v_cols[i], v_cols[j], v_corr.iloc[i,j]))\n",
    "\n",
    "\n",
    "print(f'Highly correlated pairs |r| > 95 : {len(high_corr_pairs)}')\n",
    "print('\\nSample pairs :')\n",
    "for f1, f2, r in high_corr_pairs[:10]:\n",
    "    print(f'{f1} - {f2} : {r:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17754522",
   "metadata": {},
   "source": [
    "## insight : V-features correlations clusters\n",
    "\n",
    "from the correlation anyallysis , we identify dense clusters of redudancy :\n",
    "- many v-features paris exceed r > 0.95 whic indicates near perfect correaltion\n",
    "- these likely com from similiar aggregation windows or realted transformations in vesta's system\n",
    "- keeping all of them adds noise and training overhead without imporving AUC\n",
    "\n",
    "pruning strategy : for each correalted pair, drop the feature with lower target correlation or higher missingness to preserve the most predictive signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a96059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify redundant features \n",
    "\n",
    "# target correaltion\n",
    "target_corr = sample.join(train['isFraud']).corr()['isFraud'].drop('isFraud').abs()\n",
    "\n",
    "# for each correalted pairm mark the one with lower target correaltionfor removal\n",
    "to_remove = set()\n",
    "for f1, f2, r in high_corr_pairs:\n",
    "    if target_corr.get(f1, 0) < target_corr.get(f2, 0):\n",
    "        to_remove.add(f1)\n",
    "    else:\n",
    "        to_remove.add(f2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
